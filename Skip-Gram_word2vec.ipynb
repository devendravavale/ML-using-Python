{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip-Gram.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "miLDJ58c73Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHw4tlZTfec6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class word2vec():\n",
        "  '''Coverts words to vectors using skip-gram method'''\n",
        "  def __init__(self,corpus,window,epochs,alpha):\n",
        "    '''Input training data and parameters: (corpus, window size, epochs, alpha (learning rate))'''\n",
        "    self.corpus = corpus\n",
        "    self.window = window\n",
        "    self.epochs = epochs\n",
        "    self.alpha = alpha\n",
        "  def preprocess(self,corpus):\n",
        "    '''Preprocess the data (converts to lower case). Outputs the preprocessed corpus and a list of vocabulary'''\n",
        "    #Convert to lower case\n",
        "    corpus = self.corpus.lower().split()\n",
        "    #build vocabulary of unique words\n",
        "    vocab = []\n",
        "    for i in corpus:\n",
        "      if not i in vocab:\n",
        "        vocab.append(i)\n",
        "    #Return preprocessed corpus and vocabulary of unique words\n",
        "    return corpus, vocab\n",
        "\n",
        "  def onehot(self):\n",
        "    '''Creates one hot vectors for every unique word in the vocabulary'''\n",
        "    #Extract vocabulary from the preprocessing function\n",
        "    vocab = self.preprocess(self.corpus)[1]\n",
        "    #Build a list of one hot vectors for the words from vocabulary\n",
        "    vec_list = []\n",
        "    for j in range(len(vocab)):\n",
        "      word_vec = np.zeros(len(vocab))\n",
        "      word_vec[j] = 1\n",
        "      vec_list.append(word_vec)\n",
        "    return vec_list\n",
        "\n",
        "  def train(self):\n",
        "    '''Trains the model and returns the final weights (word vectors)'''\n",
        "    #Extracting corpus from the preprocessing function\n",
        "    corpus = self.preprocess(self.corpus)[0]\n",
        "    #Create random weight vectors to initiate. Every word will be expressed using a 10 dimensional vector\n",
        "    self.w1 = np.random.uniform(-1,1,(len(self.preprocess(self.corpus)[1]),10))\n",
        "    self.w2 = np.random.uniform(-1,1,(10,len(self.preprocess(self.corpus)[1])))\n",
        "    #Initiate training\n",
        "    for i in range(self.epochs):\n",
        "      #Initiate loss variable\n",
        "      loss = 0\n",
        "      #Slide through each window depending on the window size and increment by the target word\n",
        "      for j, word in enumerate(corpus):\n",
        "        #Extracting the current window\n",
        "        first = j-self.window\n",
        "        last = j+self.window+1\n",
        "        if first < 0:\n",
        "          first = 0\n",
        "        if last > len(corpus):\n",
        "          last = len(corpus)\n",
        "        #Current window\n",
        "        current_window = corpus[first:last]\n",
        "        #Target word and one hot encoding for it\n",
        "        target_word = word\n",
        "        target_index = self.preprocess(self.corpus)[1].index(target_word)\n",
        "        target_vec = self.onehot()[target_index]\n",
        "        #Context words and one hot encoding for every context word\n",
        "        context_words = [word for word in current_window if word not in target_word]\n",
        "        context_vecs = []\n",
        "        for context_word in context_words:\n",
        "          context_index = self.preprocess(self.corpus)[1].index(context_word)\n",
        "          context_vecs.append(self.onehot()[context_index])\n",
        "\n",
        "        ########################################  FORWARD PASS #########################################################\n",
        "        h,u,y_pred = self.forwardpass(target_vec,self.w1,self.w2)\n",
        "\n",
        "        ######################################## BACKPROP ################################################################\n",
        "        error,d1_w1,d1_w2 = self.backprop(target_vec,context_vecs,h,u,y_pred,self.w2)\n",
        "\n",
        "        ######################################## ADJUSTING WEIGHTS ####################################################\n",
        "        self.w1 = self.w1 - self.alpha * d1_w1\n",
        "        self.w2 = self.w2 - self.alpha * d1_w2\n",
        "\n",
        "        ######################################## CALCULATE LOSS FUNCTION ##############################################\n",
        "        loss += -np.sum([u[self.preprocess(self.corpus)[1].index(word)] for word in context_words]) + len(context_words) * np.log(np.sum(np.exp(u)))\n",
        "\n",
        "      # Print loss after every epoch\n",
        "      print('Epoch',i,'loss:',loss)\n",
        "    return (self.w1)\n",
        "  def forwardpass(self, target_vec, w1, w2):\n",
        "    '''Performs the forward pass'''\n",
        "    h = np.dot(target_vec,w1)   #Hidden layer\n",
        "    u = np.dot(h,w2)     #Output layer\n",
        "    y_pred = self.softmax(u)     #Softmax output\n",
        "    return (h,u,y_pred)\n",
        "\n",
        "  def softmax(self,u):\n",
        "    '''Outputs the softmax function for a given vector'''\n",
        "    e = np.exp(u - np.max(u))\n",
        "    soft_fun = e/e.sum(axis=0)\n",
        "    return (soft_fun)\n",
        "\n",
        "  def backprop(self,target_vec,context_vecs,h,u,y_pred,w2):\n",
        "    '''Performs backpropagation'''\n",
        "    error = sum(y_pred - context_vecs)\n",
        "    d1_w2 = np.outer (h,error)   #gradient for w2\n",
        "    d1_w1 = np.outer (target_vec,np.dot(w2, error.T))  #gradient for w1\n",
        "    return(error,d1_w1,d1_w2)\n",
        "\n",
        "  def word_vec(self,word):\n",
        "    '''Outputs the word vector for a given word'''\n",
        "    word_index = self.preprocess(self.corpus)[1].index(word)\n",
        "    word_vec = self.w1[word_index]\n",
        "    return (word_vec) "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbb9YExmfize",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = 'He speaks good marathi. He is proficient in marathi.'"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHormduUgkFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_mod = word2vec(corpus,window=2,epochs=50,alpha=0.01)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W29rjkaEgWGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "4455ca9d-83bf-4feb-db61-52e04efc4dcc"
      },
      "source": [
        "weights = word2vec_mod.train()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 78.13317516963546\n",
            "Epoch 1 loss: 74.86642028848966\n",
            "Epoch 2 loss: 72.1666897516697\n",
            "Epoch 3 loss: 69.89479294729816\n",
            "Epoch 4 loss: 67.95247757888289\n",
            "Epoch 5 loss: 66.2693997466885\n",
            "Epoch 6 loss: 64.79419869970351\n",
            "Epoch 7 loss: 63.48857588541102\n",
            "Epoch 8 loss: 62.323383108856625\n",
            "Epoch 9 loss: 61.276013621501754\n",
            "Epoch 10 loss: 60.328634131940795\n",
            "Epoch 11 loss: 59.466963790391254\n",
            "Epoch 12 loss: 58.679414005898195\n",
            "Epoch 13 loss: 57.95647043398482\n",
            "Epoch 14 loss: 57.29024053222467\n",
            "Epoch 15 loss: 56.674116456183214\n",
            "Epoch 16 loss: 56.102519798570064\n",
            "Epoch 17 loss: 55.57070544049834\n",
            "Epoch 18 loss: 55.07460882159977\n",
            "Epoch 19 loss: 54.610725611471004\n",
            "Epoch 20 loss: 54.17601592153372\n",
            "Epoch 21 loss: 53.7678273607602\n",
            "Epoch 22 loss: 53.38383274490188\n",
            "Epoch 23 loss: 53.02197933187329\n",
            "Epoch 24 loss: 52.68044721632512\n",
            "Epoch 25 loss: 52.35761506745395\n",
            "Epoch 26 loss: 52.05203179854349\n",
            "Epoch 27 loss: 51.762393057480125\n",
            "Epoch 28 loss: 51.487521654145205\n",
            "Epoch 29 loss: 51.22635121382343\n",
            "Epoch 30 loss: 50.977912480114504\n",
            "Epoch 31 loss: 50.74132179658142\n",
            "Epoch 32 loss: 50.51577138081556\n",
            "Epoch 33 loss: 50.30052107295289\n",
            "Epoch 34 loss: 50.09489129665376\n",
            "Epoch 35 loss: 49.89825701682987\n",
            "Epoch 36 loss: 49.71004251689985\n",
            "Epoch 37 loss: 49.52971685049613\n",
            "Epoch 38 loss: 49.356789849398744\n",
            "Epoch 39 loss: 49.190808591859515\n",
            "Epoch 40 loss: 49.0313542540627\n",
            "Epoch 41 loss: 48.878039282793935\n",
            "Epoch 42 loss: 48.73050483992495\n",
            "Epoch 43 loss: 48.588418479478435\n",
            "Epoch 44 loss: 48.451472026175914\n",
            "Epoch 45 loss: 48.31937963081782\n",
            "Epoch 46 loss: 48.1918759828837\n",
            "Epoch 47 loss: 48.068714664627606\n",
            "Epoch 48 loss: 47.949666633898524\n",
            "Epoch 49 loss: 47.83451882513056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahOZKiu3hHgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "025bef10-4dfe-4d02-b31b-5b6d63dceb66"
      },
      "source": [
        "word2vec_mod.word_vec('good')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.99304862, -0.24612498, -0.34696777, -0.51852856,  0.27075523,\n",
              "       -0.06650622, -0.38466629, -0.51190019, -0.27699964, -0.50457998])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS7YqLUU2imz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}